{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://keras.io/examples/rl/ddpg_pendulum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of State Space ->  3\n",
      "Size of Action Space ->  1\n",
      "Max Value of Action ->  2.0\n",
      "Min Value of Action ->  -2.0\n"
     ]
    }
   ],
   "source": [
    "problem = \"Pendulum-v0\"\n",
    "env = gym.make(problem)\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "num_actions = env.action_space.shape[0]\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        # Num of tuples to train on.\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Its tells us num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "\n",
    "    # Takes (s,a,r,s') obervation tuple as input\n",
    "    def record(self, obs_tuple):\n",
    "        # Set index to zero if buffer_capacity is exceeded,\n",
    "        # replacing old records\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows\n",
    "    # TensorFlow to build a static graph out of the logic and computations in our function.\n",
    "    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n",
    "    @tf.function\n",
    "    def update(\n",
    "        self, state_batch, action_batch, reward_batch, next_state_batch,\n",
    "    ):\n",
    "        # Training and updating Actor & Critic networks.\n",
    "        # See Pseudo Code.\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = target_actor(next_state_batch, training=True)\n",
    "            y = reward_batch + gamma * target_critic(\n",
    "                [next_state_batch, target_actions], training=True\n",
    "            )\n",
    "            critic_value = critic_model([state_batch, action_batch], training=True)\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = actor_model(state_batch, training=True)\n",
    "            critic_value = critic_model([state_batch, actions], training=True)\n",
    "            # Used `-value` as we want to maximize the value given\n",
    "            # by the critic for our actions\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "    # We compute the loss and update parameters\n",
    "    def learn(self):\n",
    "        # Get sampling range\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        # Convert to tensors\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "\n",
    "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
    "\n",
    "\n",
    "# This update target parameters slowly\n",
    "# Based on rate `tau`, which is much less than one.\n",
    "@tf.function\n",
    "def update_target(target_weights, weights, tau):\n",
    "    for (a, b) in zip(target_weights, weights):\n",
    "        a.assign(b * tau + a * (1 - tau))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actor():\n",
    "    # Initialize weights between -3e-3 and 3-e3\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "\n",
    "    inputs = layers.Input(shape=(num_states,))\n",
    "    out = layers.Dense(256, activation=\"relu\")(inputs)\n",
    "    out = layers.Dense(256, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(1, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "\n",
    "    # Our upper bound is 2.0 for Pendulum.\n",
    "    outputs = outputs * upper_bound\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_critic():\n",
    "    # State as input\n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "\n",
    "    # Action as input\n",
    "    action_input = layers.Input(shape=(num_actions))\n",
    "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "    # Both are passed through seperate layer before concatenating\n",
    "    concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "    out = layers.Dense(256, activation=\"relu\")(concat)\n",
    "    out = layers.Dense(256, activation=\"relu\")(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "\n",
    "    # Outputs single value for give state-action\n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object):\n",
    "    sampled_actions = tf.squeeze(actor_model(state))\n",
    "    noise = noise_object()\n",
    "    # Adding noise to action\n",
    "    sampled_actions = sampled_actions.numpy() + noise\n",
    "\n",
    "    # We make sure action is within bounds\n",
    "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "    return [np.squeeze(legal_action)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2\n",
    "ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
    "\n",
    "actor_model = get_actor()\n",
    "critic_model = get_critic()\n",
    "\n",
    "target_actor = get_actor()\n",
    "target_critic = get_critic()\n",
    "\n",
    "# Making the weights equal initially\n",
    "target_actor.set_weights(actor_model.get_weights())\n",
    "target_critic.set_weights(critic_model.get_weights())\n",
    "\n",
    "# Learning rate for actor-critic models\n",
    "critic_lr = 0.002\n",
    "actor_lr = 0.001\n",
    "\n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "total_episodes = 100\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "# Used to update target networks\n",
    "tau = 0.005\n",
    "\n",
    "buffer = Buffer(50000, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 0 * Avg Reward is ==> -1623.67607229662\n",
      "Episode * 1 * Avg Reward is ==> -1358.3469843578073\n",
      "Episode * 2 * Avg Reward is ==> -1527.0539730704195\n",
      "Episode * 3 * Avg Reward is ==> -1577.0925765764027\n",
      "Episode * 4 * Avg Reward is ==> -1571.1708174190899\n",
      "Episode * 5 * Avg Reward is ==> -1531.5964076694706\n",
      "Episode * 6 * Avg Reward is ==> -1529.6225093588002\n",
      "Episode * 7 * Avg Reward is ==> -1516.9145733965788\n",
      "Episode * 8 * Avg Reward is ==> -1493.5647471879456\n",
      "Episode * 9 * Avg Reward is ==> -1478.2057872042485\n",
      "Episode * 10 * Avg Reward is ==> -1479.4059962347985\n",
      "Episode * 11 * Avg Reward is ==> -1471.1581089320343\n",
      "Episode * 12 * Avg Reward is ==> -1471.7785163626702\n",
      "Episode * 13 * Avg Reward is ==> -1448.8352420903427\n",
      "Episode * 14 * Avg Reward is ==> -1418.6606783949487\n",
      "Episode * 15 * Avg Reward is ==> -1407.1337181752142\n",
      "Episode * 16 * Avg Reward is ==> -1400.6388748695572\n",
      "Episode * 17 * Avg Reward is ==> -1368.2366394836774\n",
      "Episode * 18 * Avg Reward is ==> -1338.1864390443623\n",
      "Episode * 19 * Avg Reward is ==> -1271.4969407737726\n",
      "Episode * 20 * Avg Reward is ==> -1217.2594472906549\n",
      "Episode * 21 * Avg Reward is ==> -1167.7528885786871\n",
      "Episode * 22 * Avg Reward is ==> -1127.8062771225377\n",
      "Episode * 23 * Avg Reward is ==> -1086.2010442416586\n",
      "Episode * 24 * Avg Reward is ==> -1042.7973101188334\n",
      "Episode * 25 * Avg Reward is ==> -1007.6187372499745\n",
      "Episode * 26 * Avg Reward is ==> -984.8988132382668\n",
      "Episode * 27 * Avg Reward is ==> -963.4999202693542\n",
      "Episode * 28 * Avg Reward is ==> -939.062344623914\n",
      "Episode * 29 * Avg Reward is ==> -907.9774350929622\n",
      "Episode * 30 * Avg Reward is ==> -878.9018658787924\n",
      "Episode * 31 * Avg Reward is ==> -855.5015210475677\n",
      "Episode * 32 * Avg Reward is ==> -840.1533240450046\n",
      "Episode * 33 * Avg Reward is ==> -819.1046841687753\n",
      "Episode * 34 * Avg Reward is ==> -802.5625143086875\n",
      "Episode * 35 * Avg Reward is ==> -783.5276126864043\n",
      "Episode * 36 * Avg Reward is ==> -765.8232821482763\n",
      "Episode * 37 * Avg Reward is ==> -749.0537588141921\n",
      "Episode * 38 * Avg Reward is ==> -736.1737022477921\n",
      "Episode * 39 * Avg Reward is ==> -720.8755159374209\n",
      "Episode * 40 * Avg Reward is ==> -683.3764820627493\n",
      "Episode * 41 * Avg Reward is ==> -659.2273072422846\n",
      "Episode * 42 * Avg Reward is ==> -615.7428702556933\n",
      "Episode * 43 * Avg Reward is ==> -575.700956459661\n",
      "Episode * 44 * Avg Reward is ==> -539.92421092332\n",
      "Episode * 45 * Avg Reward is ==> -509.6358463031702\n",
      "Episode * 46 * Avg Reward is ==> -474.86464437420017\n",
      "Episode * 47 * Avg Reward is ==> -442.1137006130243\n",
      "Episode * 48 * Avg Reward is ==> -412.4789846976837\n",
      "Episode * 49 * Avg Reward is ==> -385.63533841635666\n",
      "Episode * 50 * Avg Reward is ==> -351.46691989794164\n",
      "Episode * 51 * Avg Reward is ==> -320.12941758729005\n",
      "Episode * 52 * Avg Reward is ==> -292.5511606648064\n",
      "Episode * 53 * Avg Reward is ==> -270.56873923228017\n",
      "Episode * 54 * Avg Reward is ==> -248.67161559145455\n",
      "Episode * 55 * Avg Reward is ==> -220.75298625989768\n",
      "Episode * 56 * Avg Reward is ==> -194.9533661915912\n",
      "Episode * 57 * Avg Reward is ==> -177.57937237255308\n",
      "Episode * 58 * Avg Reward is ==> -160.81519596564738\n",
      "Episode * 59 * Avg Reward is ==> -160.7684799775084\n",
      "Episode * 60 * Avg Reward is ==> -160.6682996895358\n",
      "Episode * 61 * Avg Reward is ==> -160.54415539831112\n",
      "Episode * 62 * Avg Reward is ==> -157.28049953647093\n",
      "Episode * 63 * Avg Reward is ==> -159.85186336605003\n",
      "Episode * 64 * Avg Reward is ==> -162.94333129994934\n",
      "Episode * 65 * Avg Reward is ==> -168.06427290013386\n",
      "Episode * 66 * Avg Reward is ==> -161.32019817263614\n",
      "Episode * 67 * Avg Reward is ==> -154.77578639055127\n",
      "Episode * 68 * Avg Reward is ==> -151.505638208246\n",
      "Episode * 69 * Avg Reward is ==> -154.46475479024267\n",
      "Episode * 70 * Avg Reward is ==> -154.37621686718052\n",
      "Episode * 71 * Avg Reward is ==> -159.75793904935918\n",
      "Episode * 72 * Avg Reward is ==> -154.04501660740212\n",
      "Episode * 73 * Avg Reward is ==> -157.05363401034947\n",
      "Episode * 74 * Avg Reward is ==> -157.73600803787673\n",
      "Episode * 75 * Avg Reward is ==> -157.8670155406893\n",
      "Episode * 76 * Avg Reward is ==> -154.68718816076498\n",
      "Episode * 77 * Avg Reward is ==> -154.50395710398706\n",
      "Episode * 78 * Avg Reward is ==> -154.47951366433114\n",
      "Episode * 79 * Avg Reward is ==> -151.406257310517\n",
      "Episode * 80 * Avg Reward is ==> -154.6724203467782\n",
      "Episode * 81 * Avg Reward is ==> -154.6691225838246\n",
      "Episode * 82 * Avg Reward is ==> -154.49594619207932\n",
      "Episode * 83 * Avg Reward is ==> -160.22214445497363\n",
      "Episode * 84 * Avg Reward is ==> -160.50929150228075\n",
      "Episode * 85 * Avg Reward is ==> -163.01974611624382\n",
      "Episode * 86 * Avg Reward is ==> -162.8447708957177\n",
      "Episode * 87 * Avg Reward is ==> -162.83642687563236\n",
      "Episode * 88 * Avg Reward is ==> -163.09186386127507\n",
      "Episode * 89 * Avg Reward is ==> -159.64765307048188\n",
      "Episode * 90 * Avg Reward is ==> -156.6226149286371\n",
      "Episode * 91 * Avg Reward is ==> -156.57093632508813\n",
      "Episode * 92 * Avg Reward is ==> -150.04631254428506\n",
      "Episode * 93 * Avg Reward is ==> -149.11369009513956\n",
      "Episode * 94 * Avg Reward is ==> -151.9363078899869\n",
      "Episode * 95 * Avg Reward is ==> -152.03793251059807\n",
      "Episode * 96 * Avg Reward is ==> -151.4659113461824\n",
      "Episode * 97 * Avg Reward is ==> -151.4771006120311\n",
      "Episode * 98 * Avg Reward is ==> -148.33711893995724\n",
      "Episode * 99 * Avg Reward is ==> -151.4418063300915\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To store reward history of each episode\n",
    "ep_reward_list = []\n",
    "# To store average reward history of last few episodes\n",
    "avg_reward_list = []\n",
    "\n",
    "# Takes about 4 min to train\n",
    "for ep in range(total_episodes):\n",
    "\n",
    "    prev_state = env.reset()\n",
    "    episodic_reward = 0\n",
    "\n",
    "    while True:\n",
    "        # Uncomment this to see the Actor in action\n",
    "        # But not in a python notebook.\n",
    "        # env.render()\n",
    "\n",
    "        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "\n",
    "        action = policy(tf_prev_state, ou_noise)\n",
    "        # Recieve state and reward from environment.\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        buffer.record((prev_state, action, reward, state))\n",
    "        episodic_reward += reward\n",
    "\n",
    "        buffer.learn()\n",
    "        update_target(target_actor.variables, actor_model.variables, tau)\n",
    "        update_target(target_critic.variables, critic_model.variables, tau)\n",
    "\n",
    "        # End this episode when `done` is True\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        prev_state = state\n",
    "\n",
    "    ep_reward_list.append(episodic_reward)\n",
    "\n",
    "    # Mean of last 40 episodes\n",
    "    avg_reward = np.mean(ep_reward_list[-40:])\n",
    "    print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "    avg_reward_list.append(avg_reward)\n",
    "\n",
    "# Plotting graph\n",
    "# Episodes versus Avg. Rewards\n",
    "plt.plot(avg_reward_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights\n",
    "actor_model.save_weights(\"pendulum_actor.h5\")\n",
    "critic_model.save_weights(\"pendulum_critic.h5\")\n",
    "\n",
    "target_actor.save_weights(\"pendulum_target_actor.h5\")\n",
    "target_critic.save_weights(\"pendulum_target_critic.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
