{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-inf -inf -inf -inf -inf -inf -inf -inf]\n",
      "[inf inf inf inf inf inf inf inf]\n",
      "[-1. -1.]\n",
      "[1. 1.]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "state_low = env.observation_space.low\n",
    "state_high = env.observation_space.high\n",
    "action_low = env.action_space.low \n",
    "action_high = env.action_space.high\n",
    "print(state_low)\n",
    "print(state_high)\n",
    "print(action_low)\n",
    "print(action_high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(env.action_space.high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBuffer():\n",
    "    def __init__(self, maxsize, statedim, naction):\n",
    "        self.cnt = 0\n",
    "        self.maxsize = maxsize\n",
    "        self.state_memory = np.zeros((maxsize, *statedim), dtype=np.float32)\n",
    "        self.action_memory = np.zeros((maxsize, naction), dtype=np.float32)\n",
    "        self.reward_memory = np.zeros((maxsize,), dtype=np.float32)\n",
    "        self.next_state_memory = np.zeros((maxsize, *statedim), dtype=np.float32)\n",
    "        self.done_memory = np.zeros((maxsize,), dtype= np.bool)\n",
    "\n",
    "    def storexp(self, state, next_state, action, done, reward):\n",
    "        index = self.cnt % self.maxsize\n",
    "        self.state_memory[index] = state\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.next_state_memory[index] = next_state\n",
    "        self.done_memory[index] = 1 - int(done)\n",
    "        self.cnt += 1\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        max_mem = min(self.cnt, self.maxsize)\n",
    "        batch = np.random.choice(max_mem, batch_size, replace= False)  \n",
    "        states = self.state_memory[batch]\n",
    "        next_states = self.next_state_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        dones = self.done_memory[batch]\n",
    "        return states, next_states, rewards, actions, dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.f1 = tf.keras.layers.Dense(512, activation='relu')\n",
    "        self.f2 = tf.keras.layers.Dense(512, activation='relu')\n",
    "        self.v =  tf.keras.layers.Dense(1, activation=None)\n",
    "\n",
    "    def call(self, inputstate, action):\n",
    "        x = self.f1(tf.concat([inputstate, action], axis=1))\n",
    "        x = self.f2(x)\n",
    "        x = self.v(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self, no_action):\n",
    "        super(Actor, self).__init__()    \n",
    "        self.f1 = tf.keras.layers.Dense(512, activation='relu')\n",
    "        self.f2 = tf.keras.layers.Dense(512, activation='relu')\n",
    "        self.mu =  tf.keras.layers.Dense(no_action, activation='tanh')\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.f1(state)\n",
    "        x = self.f2(x)\n",
    "        x = self.mu(x)  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, n_action= len(env.action_space.high)):\n",
    "        self.actor_main = Actor(n_action)\n",
    "        self.actor_target = Actor(n_action)\n",
    "        self.critic_main = Critic()\n",
    "        self.critic_target = Critic()\n",
    "        self.batch_size = 64\n",
    "        self.n_actions = len(env.action_space.high)\n",
    "        self.a_opt = tf.keras.optimizers.Adam(1e-4)\n",
    "        # self.actor_target = tf.keras.optimizers.Adam(.001)\n",
    "        self.c_opt = tf.keras.optimizers.Adam(1e-4)\n",
    "        # self.critic_target = tf.keras.optimizers.Adam(.002)\n",
    "        self.memory = RBuffer(1_00_000, env.observation_space.shape, len(env.action_space.high))\n",
    "        self.trainstep = 0\n",
    "        self.replace = 5\n",
    "        self.gamma = 0.99\n",
    "        self.min_action = env.action_space.low[0]\n",
    "        self.max_action = env.action_space.high[0]\n",
    "        self.tau = 0.005\n",
    "        self.actor_target.compile(optimizer=self.a_opt)\n",
    "        self.critic_target.compile(optimizer=self.c_opt)\n",
    "\n",
    "\n",
    "    def act(self, state, evaluate=False):\n",
    "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "        actions = self.actor_main(state)\n",
    "        if not evaluate:\n",
    "            actions += tf.random.normal(shape=[self.n_actions], mean=0.0, stddev=0.1)\n",
    "\n",
    "        actions = self.max_action * (tf.clip_by_value(actions, self.min_action, self.max_action))\n",
    "        #print(actions)\n",
    "        return actions[0]\n",
    "\n",
    "\n",
    "    def savexp(self,state, next_state, action, done, reward):\n",
    "        self.memory.storexp(state, next_state, action, done, reward)\n",
    "\n",
    "    def update_target(self, tau=None):\n",
    "\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "\n",
    "        weights1 = []\n",
    "        targets1 = self.actor_target.weights\n",
    "        for i, weight in enumerate(self.actor_main.weights):\n",
    "            weights1.append(weight * tau + targets1[i]*(1-tau))\n",
    "        self.actor_target.set_weights(weights1)\n",
    "\n",
    "        weights2 = []\n",
    "        targets2 = self.critic_target.weights\n",
    "        for i, weight in enumerate(self.critic_main.weights):\n",
    "            weights2.append(weight * tau + targets2[i]*(1-tau))\n",
    "        self.critic_target.set_weights(weights2)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        if self.memory.cnt < self.batch_size:\n",
    "            return \n",
    "\n",
    "\n",
    "        states, next_states, rewards, actions, dones = self.memory.sample(self.batch_size)\n",
    "\n",
    "        states = tf.convert_to_tensor(states, dtype= tf.float32)\n",
    "        next_states = tf.convert_to_tensor(next_states, dtype= tf.float32)\n",
    "        rewards = tf.convert_to_tensor(rewards, dtype= tf.float32)\n",
    "        actions = tf.convert_to_tensor(actions, dtype= tf.float32)\n",
    "        #dones = tf.convert_to_tensor(dones, dtype= tf.bool)\n",
    "\n",
    "        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "\n",
    "            target_actions = self.actor_target(next_states)\n",
    "            target_next_state_values = tf.squeeze(self.critic_target(next_states, target_actions), 1)\n",
    "            critic_value = tf.squeeze(self.critic_main(states, actions), 1)\n",
    "            target_values = rewards + self.gamma * target_next_state_values * dones\n",
    "            critic_loss = tf.keras.losses.MSE(target_values, critic_value)\n",
    "\n",
    "            new_policy_actions = self.actor_main(states)\n",
    "            actor_loss = -self.critic_main(states, new_policy_actions)\n",
    "            actor_loss = tf.math.reduce_mean(actor_loss)\n",
    "\n",
    "        grads1 = tape1.gradient(actor_loss, self.actor_main.trainable_variables)\n",
    "        grads2 = tape2.gradient(critic_loss, self.critic_main.trainable_variables)\n",
    "        self.a_opt.apply_gradients(zip(grads1, self.actor_main.trainable_variables))\n",
    "        self.c_opt.apply_gradients(zip(grads2, self.critic_main.trainable_variables))\n",
    "\n",
    "        #if self.trainstep % self.replace == 0:\n",
    "        self.update_target()\n",
    "\n",
    "        self.trainstep +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward after 0 steps is -182.18068361789182 and avg reward is -182.18068361789182\n",
      "total reward after 1 steps is -34.65772060524867 and avg reward is -108.41920211157024\n",
      "total reward after 2 steps is -677.5523053950737 and avg reward is -298.13023653940473\n",
      "total reward after 3 steps is -1042.6865012443532 and avg reward is -484.26930271564186\n",
      "total reward after 4 steps is -825.9514400091957 and avg reward is -552.6057301743526\n",
      "total reward after 5 steps is -316.6279091949686 and avg reward is -513.2760933444553\n",
      "total reward after 6 steps is -229.70907768839515 and avg reward is -472.7665196793038\n",
      "total reward after 7 steps is -284.37523514949874 and avg reward is -449.2176091130782\n",
      "total reward after 8 steps is -430.49598666180873 and avg reward is -447.1374288407149\n",
      "total reward after 9 steps is -412.7043219516426 and avg reward is -443.69411815180763\n",
      "total reward after 10 steps is -93.97266970437174 and avg reward is -411.90125920204076\n",
      "total reward after 11 steps is -218.55344049690234 and avg reward is -395.78894097661254\n",
      "total reward after 12 steps is -215.07590139658402 and avg reward is -381.887937931995\n",
      "total reward after 13 steps is -291.361946902918 and avg reward is -375.42179571563236\n",
      "total reward after 14 steps is -374.71361793703716 and avg reward is -375.374583863726\n",
      "total reward after 15 steps is -270.23231169984695 and avg reward is -368.80319185348355\n",
      "total reward after 16 steps is -249.72210129012095 and avg reward is -361.7984218203446\n",
      "total reward after 17 steps is -173.8024113863524 and avg reward is -351.3541990184561\n",
      "total reward after 18 steps is -325.50198557103295 and avg reward is -349.99355620543383\n",
      "total reward after 19 steps is -331.08985701805454 and avg reward is -349.04837124606485\n",
      "total reward after 20 steps is -589.0331300844462 and avg reward is -360.47621690503536\n",
      "total reward after 21 steps is -389.7660449227522 and avg reward is -361.8075727240225\n",
      "total reward after 22 steps is -84.96172772651931 and avg reward is -349.7707968545659\n",
      "total reward after 23 steps is -311.4504168581335 and avg reward is -348.1741143547145\n",
      "total reward after 24 steps is -566.3273154422568 and avg reward is -356.9002423982162\n",
      "total reward after 25 steps is -538.500790314672 and avg reward is -363.88487885654143\n",
      "total reward after 26 steps is -273.88783503663177 and avg reward is -360.5516550113596\n",
      "total reward after 27 steps is -138.15415460865214 and avg reward is -352.60888713983434\n",
      "total reward after 28 steps is -250.86601234295085 and avg reward is -349.10051214683835\n",
      "total reward after 29 steps is -275.4142726801473 and avg reward is -346.64430416461533\n",
      "total reward after 30 steps is -246.72642593145198 and avg reward is -343.42114680225524\n",
      "total reward after 31 steps is -290.0476458479104 and avg reward is -341.75322489743195\n",
      "total reward after 32 steps is -287.74607055384865 and avg reward is -340.1166444627779\n",
      "total reward after 33 steps is -319.3430665266709 and avg reward is -339.5056568764218\n",
      "total reward after 34 steps is -354.94548251103583 and avg reward is -339.94679475169653\n",
      "total reward after 35 steps is -282.4622471973627 and avg reward is -338.3500017640761\n",
      "total reward after 36 steps is -273.2844416564067 and avg reward is -336.5914731125175\n",
      "total reward after 37 steps is -177.2472177004983 and avg reward is -332.39820323325387\n",
      "total reward after 38 steps is -219.93649544629216 and avg reward is -329.51456970025487\n",
      "total reward after 39 steps is -39.34983446780442 and avg reward is -322.2604513194436\n",
      "total reward after 40 steps is -454.36326759498877 and avg reward is -325.4824712286032\n",
      "total reward after 41 steps is -399.4355756646883 and avg reward is -327.2432594294624\n",
      "total reward after 42 steps is -240.8031061371563 and avg reward is -325.2330233063855\n",
      "total reward after 43 steps is -4.408649355831557 and avg reward is -317.94156026205474\n",
      "total reward after 44 steps is -165.13697136757375 and avg reward is -314.5459027310663\n",
      "total reward after 45 steps is 5.79655891606609 and avg reward is -307.5819361735199\n",
      "total reward after 46 steps is -218.70619800818088 and avg reward is -305.6909630210659\n",
      "total reward after 47 steps is -46.21457296812572 and avg reward is -300.285204894963\n",
      "total reward after 48 steps is -51.49367396043873 and avg reward is -295.2078267126257\n",
      "total reward after 49 steps is -191.52266033212334 and avg reward is -293.13412338501564\n",
      "total reward after 50 steps is -106.50363153656195 and avg reward is -289.47470197622243\n",
      "total reward after 51 steps is -98.72379090052843 and avg reward is -285.8064152247668\n",
      "total reward after 52 steps is -184.7354798342754 and avg reward is -283.8994164438141\n",
      "total reward after 53 steps is -156.10531325769952 and avg reward is -281.5328589774046\n",
      "total reward after 54 steps is -166.06245224228337 and avg reward is -279.43339703676605\n",
      "total reward after 55 steps is -237.45066788852267 and avg reward is -278.6837054448331\n",
      "total reward after 56 steps is -176.6825233119392 and avg reward is -276.894211021449\n",
      "total reward after 57 steps is -208.24819185887327 and avg reward is -275.7106589669218\n",
      "total reward after 58 steps is -39.73893496030272 and avg reward is -271.71113822104695\n",
      "total reward after 59 steps is 20.846754198187682 and avg reward is -266.83517334739304\n",
      "total reward after 60 steps is -154.43665448263135 and avg reward is -264.9925746774789\n",
      "total reward after 61 steps is -165.11315660184817 and avg reward is -263.3816163214203\n",
      "total reward after 62 steps is -138.12395115907321 and avg reward is -261.3933994140815\n",
      "total reward after 63 steps is -128.15638816134887 and avg reward is -259.3115711132575\n",
      "total reward after 64 steps is -4.804464146974923 and avg reward is -255.3960771599301\n",
      "total reward after 65 steps is -58.755233358049985 and avg reward is -252.41667043565917\n",
      "total reward after 66 steps is -62.22151834738043 and avg reward is -249.5779368224013\n",
      "total reward after 67 steps is -228.19900170161898 and avg reward is -249.2635407176839\n",
      "total reward after 68 steps is -71.49265169193717 and avg reward is -246.6871510216586\n",
      "total reward after 69 steps is -101.27565462762861 and avg reward is -244.6098439303153\n",
      "total reward after 70 steps is -218.74608076693363 and avg reward is -244.24556557590148\n",
      "total reward after 71 steps is -58.87462091336799 and avg reward is -241.67096912225523\n",
      "total reward after 72 steps is -144.9786691912067 and avg reward is -240.34641706840523\n",
      "total reward after 73 steps is -52.62367211858363 and avg reward is -237.80962321773197\n",
      "total reward after 74 steps is -76.06230815240303 and avg reward is -235.65299235019427\n",
      "total reward after 75 steps is -55.94643208059153 and avg reward is -233.28843234664686\n",
      "total reward after 76 steps is -34.561064463169814 and avg reward is -230.70755743906926\n",
      "total reward after 77 steps is -242.54465026559234 and avg reward is -230.85931503940932\n",
      "total reward after 78 steps is -64.41785098989351 and avg reward is -228.75246106409898\n",
      "total reward after 79 steps is -34.04390424990019 and avg reward is -226.31860410392147\n",
      "total reward after 80 steps is -87.3974093139368 and avg reward is -224.60352762503277\n",
      "total reward after 81 steps is -69.06653497332776 and avg reward is -222.7067350317193\n",
      "total reward after 82 steps is -71.5689153052909 and avg reward is -220.8857974446539\n",
      "total reward after 83 steps is -53.07277215324351 and avg reward is -218.88802333404186\n",
      "total reward after 84 steps is -73.65484588130253 and avg reward is -217.17939771695077\n",
      "total reward after 85 steps is -48.49808589022746 and avg reward is -215.21798711431447\n",
      "total reward after 86 steps is -27.702938497570766 and avg reward is -213.06264172791512\n",
      "total reward after 87 steps is -61.90874176378129 and avg reward is -211.34498377377724\n",
      "total reward after 88 steps is -49.282391993107396 and avg reward is -209.52405577624162\n",
      "total reward after 89 steps is -210.01061282648456 and avg reward is -209.5294619656888\n",
      "total reward after 90 steps is -88.06260309459252 and avg reward is -208.19466131875367\n",
      "total reward after 91 steps is -229.8651195489685 and avg reward is -208.43020977777775\n",
      "total reward after 92 steps is 73.06276446676092 and avg reward is -205.4034036031053\n",
      "total reward after 93 steps is -0.874677180646259 and avg reward is -203.22756608797275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward after 94 steps is -346.26746284585647 and avg reward is -204.73324921173995\n",
      "total reward after 95 steps is -78.39901514359298 and avg reward is -203.41726760686342\n",
      "total reward after 96 steps is 5.595738681367543 and avg reward is -201.26249434616\n",
      "total reward after 97 steps is -93.47517344104718 and avg reward is -200.16262372467924\n",
      "total reward after 98 steps is -8.535559506347202 and avg reward is -198.22699681338295\n",
      "total reward after 99 steps is -127.86019728452146 and avg reward is -197.52332881809434\n",
      "total reward after 100 steps is -63.91283168530851 and avg reward is -196.34065029876848\n",
      "total reward after 101 steps is -83.19806684007324 and avg reward is -196.82605376111675\n",
      "total reward after 102 steps is -64.31135489779375 and avg reward is -190.69364425614393\n",
      "total reward after 103 steps is -103.47252269110948 and avg reward is -181.30150447061152\n",
      "total reward after 104 steps is -111.63323014395907 and avg reward is -174.15832237195912\n",
      "total reward after 105 steps is -84.28407408643761 and avg reward is -171.8348840208738\n",
      "total reward after 106 steps is -111.76591795109667 and avg reward is -170.65545242350083\n",
      "total reward after 107 steps is -103.43653130284426 and avg reward is -168.84606538503428\n",
      "total reward after 108 steps is -129.68580289026332 and avg reward is -165.83796354731888\n",
      "total reward after 109 steps is -105.36389218466897 and avg reward is -162.76455924964912\n",
      "total reward after 110 steps is -85.94986963530033 and avg reward is -162.6843312489584\n",
      "total reward after 111 steps is -63.84672203238225 and avg reward is -161.13726406431317\n",
      "total reward after 112 steps is -113.87469296794669 and avg reward is -160.12525198002683\n",
      "total reward after 113 steps is -93.7686501373347 and avg reward is -158.14931901237102\n",
      "total reward after 114 steps is -106.65291829792876 and avg reward is -155.46871201597992\n",
      "total reward after 115 steps is -46.69857879455531 and avg reward is -153.23337468692702\n",
      "total reward after 116 steps is -90.04440900361456 and avg reward is -151.63659776406195\n",
      "total reward after 117 steps is -87.31110585836761 and avg reward is -150.7716847087821\n",
      "total reward after 118 steps is -87.47866132401012 and avg reward is -148.39145146631185\n",
      "total reward after 119 steps is -96.49047323559039 and avg reward is -146.0454576284872\n",
      "total reward after 120 steps is -77.03185141158956 and avg reward is -140.92544484175866\n",
      "total reward after 121 steps is -25.80676463375533 and avg reward is -137.28585203886868\n",
      "total reward after 122 steps is -301.05744652494775 and avg reward is -139.44680922685296\n",
      "total reward after 123 steps is -155.92415928402255 and avg reward is -137.89154665111187\n",
      "total reward after 124 steps is -13.640310959896434 and avg reward is -132.36467660628824\n",
      "total reward after 125 steps is -147.70917040432298 and avg reward is -128.4567604071848\n",
      "total reward after 126 steps is -176.54157666594585 and avg reward is -127.4832978234779\n",
      "total reward after 127 steps is -182.12176299156158 and avg reward is -127.92297390730698\n",
      "total reward after 128 steps is -54.391359404065255 and avg reward is -125.95822737791815\n",
      "total reward after 129 steps is 49.50465168855402 and avg reward is -122.70903813423114\n",
      "total reward after 130 steps is -203.2978087829419 and avg reward is -122.27475196274601\n",
      "total reward after 131 steps is 7.500319792284287 and avg reward is -119.29927230634408\n",
      "total reward after 132 steps is -111.42409578318525 and avg reward is -117.53605255863744\n",
      "total reward after 133 steps is -32.17629146748546 and avg reward is -114.66438480804555\n",
      "total reward after 134 steps is -244.96709617551312 and avg reward is -113.56460094469034\n",
      "total reward after 135 steps is -55.15867182463802 and avg reward is -111.29156519096313\n",
      "total reward after 136 steps is -11.058001758929095 and avg reward is -108.66930079198835\n",
      "total reward after 137 steps is -80.26833789579317 and avg reward is -107.69951199394129\n",
      "total reward after 138 steps is -5.461661378948618 and avg reward is -105.55476365326787\n",
      "total reward after 139 steps is -73.46164167441633 and avg reward is -105.89588172533398\n",
      "total reward after 140 steps is 16.214895504937903 and avg reward is -101.19010009433472\n",
      "total reward after 141 steps is -78.17092539707174 and avg reward is -97.97745359165854\n",
      "total reward after 142 steps is -38.47815816131671 and avg reward is -95.95420411190014\n",
      "total reward after 143 steps is 27.882347629795127 and avg reward is -95.63129414204388\n",
      "total reward after 144 steps is -78.25929665216889 and avg reward is -94.76251739488984\n",
      "total reward after 145 steps is -26.249968310133895 and avg reward is -95.08298266715184\n",
      "total reward after 146 steps is -44.81136634426089 and avg reward is -93.34403435051263\n",
      "total reward after 147 steps is -18.100907730932875 and avg reward is -93.06289769814072\n",
      "total reward after 148 steps is 11.433760879520413 and avg reward is -92.4336233497411\n",
      "total reward after 149 steps is -49.096030498523575 and avg reward is -91.00935705140509\n",
      "total reward after 150 steps is 1.2422981158086965 and avg reward is -89.93189775488139\n",
      "total reward after 151 steps is -6.300864208384486 and avg reward is -89.00766848795996\n",
      "total reward after 152 steps is 16.27214319870424 and avg reward is -86.99759225763017\n",
      "total reward after 153 steps is -24.503751279541675 and avg reward is -85.6815766378486\n",
      "total reward after 154 steps is -9.302647066834849 and avg reward is -84.1139785860941\n",
      "total reward after 155 steps is -80.37265730546962 and avg reward is -82.54319848026358\n",
      "total reward after 156 steps is -104.1804051439501 and avg reward is -81.81817729858368\n",
      "total reward after 157 steps is -42.62310686526776 and avg reward is -80.16192644864763\n",
      "total reward after 158 steps is -74.30500360940354 and avg reward is -80.50758713513864\n",
      "total reward after 159 steps is -60.65334388175779 and avg reward is -81.3225881159381\n",
      "total reward after 160 steps is -44.44412380474093 and avg reward is -80.22266280915917\n",
      "total reward after 161 steps is -49.5661002408911 and avg reward is -79.06719224554962\n",
      "total reward after 162 steps is -37.91914357358586 and avg reward is -78.06514416969475\n",
      "total reward after 163 steps is -49.09808871052926 and avg reward is -77.27456117518655\n",
      "total reward after 164 steps is -30.26912457332405 and avg reward is -77.52920777945002\n",
      "total reward after 165 steps is -70.59734993637203 and avg reward is -77.64762894523325\n",
      "total reward after 166 steps is -23.316862264271577 and avg reward is -77.25858238440216\n",
      "total reward after 167 steps is -43.864529994080705 and avg reward is -75.41523766732679\n",
      "total reward after 168 steps is -55.274673698203884 and avg reward is -75.25305788738945\n",
      "total reward after 169 steps is -17.89740164256301 and avg reward is -74.4192753575388\n",
      "total reward after 170 steps is -53.230682944234424 and avg reward is -72.76412137931179\n",
      "total reward after 171 steps is -10.265014333261522 and avg reward is -72.27802531351072\n",
      "total reward after 172 steps is -42.37907086992307 and avg reward is -71.2520293302979\n",
      "total reward after 173 steps is -72.85918121388046 and avg reward is -71.45438442125086\n",
      "total reward after 174 steps is -42.00972981653589 and avg reward is -71.11385863789219\n",
      "total reward after 175 steps is -58.712471372370935 and avg reward is -71.14151903080999\n",
      "total reward after 176 steps is -47.522172300035926 and avg reward is -71.27113010917864\n",
      "total reward after 177 steps is -6.744850023345094 and avg reward is -68.91313210675618\n",
      "total reward after 178 steps is -46.467991661921914 and avg reward is -68.73363351347645\n",
      "total reward after 179 steps is -20.73274231022006 and avg reward is -68.60052189407965\n",
      "total reward after 180 steps is 21.53378867921635 and avg reward is -67.51120991414813\n",
      "total reward after 181 steps is -5.962064176561236 and avg reward is -66.88016520618045\n",
      "total reward after 182 steps is -39.75369285473793 and avg reward is -66.56201298167493\n",
      "total reward after 183 steps is -20.2266606753303 and avg reward is -66.23355186689581\n",
      "total reward after 184 steps is -31.166325943976908 and avg reward is -65.80866666752254\n",
      "total reward after 185 steps is 13.0997446189066 and avg reward is -65.1926883624312\n",
      "total reward after 186 steps is -50.738319984344145 and avg reward is -65.42304217729892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total reward after 187 steps is -2.641418792649945 and avg reward is -64.83036894758763\n",
      "total reward after 188 steps is -61.36017949016992 and avg reward is -64.95114682255824\n",
      "total reward after 189 steps is -33.078831226780984 and avg reward is -63.18182900656122\n",
      "total reward after 190 steps is -11.646625224034159 and avg reward is -62.41766922785564\n",
      "total reward after 191 steps is -48.3550453872664 and avg reward is -60.60256848623862\n",
      "total reward after 192 steps is -129.75027940789698 and avg reward is -62.6306989249852\n",
      "total reward after 193 steps is -46.69388774589336 and avg reward is -63.08889103063766\n",
      "total reward after 194 steps is -36.45474885697627 and avg reward is -59.99076389074886\n",
      "total reward after 195 steps is -61.71439650335576 and avg reward is -59.823917704346485\n",
      "total reward after 196 steps is -16.11496229437578 and avg reward is -60.04102471410391\n",
      "total reward after 197 steps is -46.93769647213443 and avg reward is -59.57564994441478\n",
      "total reward after 198 steps is -38.37638928627945 and avg reward is -59.87405824221411\n",
      "total reward after 199 steps is -27.27304967010485 and avg reward is -58.868186766069954\n",
      "total reward after 200 steps is -21.3233454154522 and avg reward is -58.4422919033714\n",
      "total reward after 201 steps is 20.335912967967857 and avg reward is -57.40695210529099\n",
      "total reward after 202 steps is -34.93847083834576 and avg reward is -57.11322326469651\n",
      "total reward after 203 steps is -59.53937126992004 and avg reward is -56.673891750484614\n",
      "total reward after 204 steps is -425.2492689377315 and avg reward is -59.810052138422336\n",
      "total reward after 205 steps is -38.77877762776849 and avg reward is -59.35499917383564\n",
      "total reward after 206 steps is 47.35857582197435 and avg reward is -57.76375423610492\n",
      "total reward after 207 steps is -142.36672260573712 and avg reward is -58.15305614913385\n",
      "total reward after 208 steps is -118.2112860383744 and avg reward is -58.03831098061496\n",
      "total reward after 209 steps is -110.810274689255 and avg reward is -58.09277480566082\n",
      "total reward after 210 steps is -129.5416696555319 and avg reward is -58.52869280586314\n",
      "total reward after 211 steps is -84.54084200849458 and avg reward is -58.73563400562426\n",
      "total reward after 212 steps is -51.94936525426234 and avg reward is -58.11638072848743\n",
      "total reward after 213 steps is -101.80296804228607 and avg reward is -58.19672390753694\n",
      "total reward after 214 steps is -124.03231203361969 and avg reward is -58.37051784489385\n",
      "total reward after 215 steps is 31.647382818801706 and avg reward is -57.58705822876027\n",
      "total reward after 216 steps is 58.97917670995582 and avg reward is -56.096822371624576\n",
      "total reward after 217 steps is -88.21589655941443 and avg reward is -56.105870278635045\n",
      "total reward after 218 steps is 190.08622667537094 and avg reward is -53.33022139864122\n",
      "total reward after 219 steps is -247.56318184047825 and avg reward is -54.84094848469011\n",
      "total reward after 220 steps is -2.3219509745148628 and avg reward is -54.093849480319356\n",
      "total reward after 221 steps is 6.258925307921137 and avg reward is -53.77319258090259\n",
      "total reward after 222 steps is 6.2432368525957145 and avg reward is -50.700185747127165\n",
      "total reward after 223 steps is -181.86920021233033 and avg reward is -50.95963615641024\n",
      "total reward after 224 steps is -41.52858953438282 and avg reward is -51.23851894215512\n",
      "total reward after 225 steps is -152.97502308186355 and avg reward is -51.291177468930506\n",
      "total reward after 226 steps is -325.44787248838486 and avg reward is -52.780240427154915\n",
      "total reward after 227 steps is -118.31717817324282 and avg reward is -52.14219457897171\n",
      "total reward after 228 steps is -12.144501974574839 and avg reward is -51.719726004676815\n",
      "total reward after 229 steps is 15.696039614764 and avg reward is -52.0578121254147\n",
      "total reward after 230 steps is -41.412586766059576 and avg reward is -50.43895990524588\n",
      "total reward after 231 steps is -30.888229367156974 and avg reward is -50.82284539684029\n",
      "total reward after 232 steps is -9.02098777486878 and avg reward is -49.798814316757124\n",
      "total reward after 233 steps is -22.514081729060344 and avg reward is -49.702192219372876\n",
      "total reward after 234 steps is 34.007310340647415 and avg reward is -46.91244815421127\n",
      "total reward after 235 steps is -29.657756782807496 and avg reward is -46.65743900379296\n",
      "total reward after 236 steps is -70.42816702670184 and avg reward is -47.25114065647069\n",
      "total reward after 237 steps is -37.8570985473274 and avg reward is -46.82702826298603\n",
      "total reward after 238 steps is -768.587158270671 and avg reward is -54.45828323190326\n",
      "total reward after 239 steps is 22.277133006646256 and avg reward is -53.500895485092634\n",
      "total reward after 240 steps is -72.22926259024649 and avg reward is -54.38533706604448\n",
      "total reward after 241 steps is -12.154523980404358 and avg reward is -53.72517305187781\n",
      "total reward after 242 steps is 221.41597014393736 and avg reward is -51.126231768825264\n",
      "total reward after 243 steps is -8.517912611755214 and avg reward is -51.49023437124076\n",
      "total reward after 244 steps is 234.09956704965032 and avg reward is -48.36664573422257\n",
      "total reward after 245 steps is 83.62851289933641 and avg reward is -47.267860922127866\n",
      "total reward after 246 steps is 273.25991246247077 and avg reward is -44.087148134060556\n",
      "total reward after 247 steps is 201.74994556237095 and avg reward is -41.88863960112751\n",
      "total reward after 248 steps is 25.167146506971434 and avg reward is -41.751305744853006\n",
      "total reward after 249 steps is 213.98263370053405 and avg reward is -39.120519102862424\n",
      "total reward after 250 steps is 179.29090392137965 and avg reward is -37.34003304480672\n",
      "total reward after 251 steps is 51.24763384089515 and avg reward is -36.76454806431393\n",
      "total reward after 252 steps is 203.99650411595724 and avg reward is -34.88730445514139\n",
      "total reward after 253 steps is 155.05412874755132 and avg reward is -33.09172565487047\n",
      "total reward after 254 steps is 185.68732523056372 and avg reward is -31.141825931896477\n",
      "total reward after 255 steps is -47.81965353321819 and avg reward is -30.81629589417397\n",
      "total reward after 256 steps is 244.2253206666901 and avg reward is -27.33223863606757\n",
      "total reward after 257 steps is 143.8669167106554 and avg reward is -25.467338400308332\n",
      "total reward after 258 steps is -127.00969844512345 and avg reward is -25.99438534866553\n",
      "total reward after 259 steps is 109.92236552023601 and avg reward is -24.288628254645594\n",
      "total reward after 260 steps is -53.03809885528046 and avg reward is -24.37456800515099\n",
      "total reward after 261 steps is 179.4777448962056 and avg reward is -22.08412955378002\n",
      "total reward after 262 steps is 220.7494280295007 and avg reward is -19.497443837749163\n",
      "total reward after 263 steps is -93.02762259024786 and avg reward is -19.936739176546347\n",
      "total reward after 264 steps is 183.28209522038696 and avg reward is -17.801226978609233\n",
      "total reward after 265 steps is 176.59217067796448 and avg reward is -15.329331772465869\n",
      "total reward after 266 steps is -45.8915055997757 and avg reward is -15.55507820582091\n",
      "total reward after 267 steps is 74.51998505893357 and avg reward is -14.371233055290768\n",
      "total reward after 268 steps is 284.1028869769601 and avg reward is -10.977457448539125\n",
      "total reward after 269 steps is 182.71559631515657 and avg reward is -8.971327468961933\n"
     ]
    }
   ],
   "source": [
    "with tf.device('GPU:0'):\n",
    "    tf.random.set_seed(336699)\n",
    "    agent = Agent(2)\n",
    "    episods = 2000\n",
    "    ep_reward = []\n",
    "    total_avgr = []\n",
    "    target = False\n",
    "\n",
    "    for s in range(episods):\n",
    "        if target == True:\n",
    "            break\n",
    "        total_reward = 0 \n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.savexp(state, next_state, action, done, reward)\n",
    "            agent.train()\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                ep_reward.append(total_reward)\n",
    "                avg_reward = np.mean(ep_reward[-100:])\n",
    "                total_avgr.append(avg_reward)\n",
    "                print(\"total reward after {} steps is {} and avg reward is {}\".format(s, total_reward, avg_reward))\n",
    "                if int(avg_reward) == 200:\n",
    "                    target = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = [i  for i in range(len(avg_rewards_list))]\n",
    "plt.plot( range(len(avg_rewards_list)),avg_rewards_list,'b')\n",
    "plt.title(\"Avg Test Aeward Vs Test Episods\")\n",
    "plt.xlabel(\"Test Episods\")\n",
    "plt.ylabel(\"Average Test Reward\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward = 0\n",
    "state = env.reset()\n",
    "while not done:\n",
    "    action = agent.act(state, True)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    state = next_state\n",
    "    total_reward += reward\n",
    "    if done:\n",
    "        print(total_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
