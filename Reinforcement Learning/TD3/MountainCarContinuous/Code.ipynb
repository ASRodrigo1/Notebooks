{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-ef76775f4c1a>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "Is GPU available? True\n",
      "TF version: 2.3.1\n",
      "Keras version: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Is GPU available?\", tf.test.is_gpu_available())\n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"Keras version:\", tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MSE\n",
    "\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.f1 = tf.keras.layers.Dense(512, activation='relu')\n",
    "        self.f2 = tf.keras.layers.Dense(512, activation='relu')\n",
    "        self.v =  tf.keras.layers.Dense(1, activation=None)\n",
    "\n",
    "    def call(self, inputstate, action):\n",
    "        x = self.f1(tf.concat([inputstate, action], axis=1))\n",
    "        x = self.f2(x)\n",
    "        x = self.v(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self, no_action):\n",
    "        super(Actor, self).__init__()    \n",
    "        self.f1 = tf.keras.layers.Dense(512, activation='relu')\n",
    "        self.f2 = tf.keras.layers.Dense(512, activation='relu')\n",
    "        self.mu =  tf.keras.layers.Dense(no_action, activation='tanh')\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.f1(state)\n",
    "        x = self.f2(x)\n",
    "        x = self.mu(x)  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, n_actions, min_action, max_action):\n",
    "        self.actor_main = Actor(n_actions)\n",
    "        self.actor_target = Actor(n_actions)\n",
    "        self.critic_main = Critic()\n",
    "        self.critic_main2 = Critic()\n",
    "        self.critic_target = Critic()\n",
    "        self.critic_target2 = Critic()\n",
    "        self.batch_size = 64\n",
    "        self.n_actions = n_actions\n",
    "        self.a_opt = tf.keras.optimizers.Adam(0.001)\n",
    "        self.c_opt1 = tf.keras.optimizers.Adam(0.002)\n",
    "        self.c_opt2 = tf.keras.optimizers.Adam(0.002)\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.trainstep = 0\n",
    "        self.gamma = 0.99\n",
    "        self.min_action = min_action\n",
    "        self.max_action = max_action\n",
    "        self.actor_update_steps = 2\n",
    "        self.warmup = 200\n",
    "        self.actor_target.compile(optimizer=self.a_opt)\n",
    "        self.critic_target.compile(optimizer=self.c_opt1)\n",
    "        self.critic_target2.compile(optimizer=self.c_opt2)\n",
    "        self.tau = 0.005\n",
    "\n",
    "\n",
    "    def act(self, state, evaluate=False):\n",
    "        if self.trainstep > self.warmup:\n",
    "            evaluate = True\n",
    "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "        actions = self.actor_main(state)\n",
    "        if not evaluate:\n",
    "            actions += tf.random.normal(shape=[self.n_actions], mean=0.0, stddev=0.1)\n",
    "\n",
    "        actions = self.max_action * (tf.clip_by_value(actions, self.min_action, self.max_action))\n",
    "        \n",
    "        return actions[0]\n",
    "    \n",
    "    def store(self, state, action, reward, n_state, done):\n",
    "        pack = [state, action, reward, n_state, 1 - int(done)]\n",
    "        self.memory.append(pack)\n",
    "    \n",
    "    def take_data(self, batch_size):\n",
    "        pack = random.sample(self.memory, batch_size)\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        n_states = []\n",
    "        dones = []\n",
    "        for i in range(batch_size):\n",
    "            states.append(pack[i][0])\n",
    "            actions.append(pack[i][1])\n",
    "            rewards.append(pack[i][2])\n",
    "            n_states.append(pack[i][3])\n",
    "            dones.append(pack[i][4])\n",
    "        return states, actions, rewards, n_states, dones\n",
    "\n",
    "    def update_target(self, tau=None):\n",
    "\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "\n",
    "        weights1 = []\n",
    "        targets1 = self.actor_target.weights\n",
    "        for i, weight in enumerate(self.actor_main.weights):\n",
    "            weights1.append(weight * tau + targets1[i]*(1-tau))\n",
    "        self.actor_target.set_weights(weights1)\n",
    "\n",
    "        weights2 = []\n",
    "        targets2 = self.critic_target.weights\n",
    "        for i, weight in enumerate(self.critic_main.weights):\n",
    "            weights2.append(weight * tau + targets2[i]*(1-tau))\n",
    "        self.critic_target.set_weights(weights2)\n",
    "\n",
    "\n",
    "        weights3 = []\n",
    "        targets3 = self.critic_target2.weights\n",
    "        for i, weight in enumerate(self.critic_main2.weights):\n",
    "            weights3.append(weight * tau + targets3[i]*(1-tau))\n",
    "        self.critic_target2.set_weights(weights3)\n",
    "    \n",
    "    def upgrade(self):\n",
    "        if len(self.memory) < 2*self.batch_size:\n",
    "            return \n",
    "\n",
    "\n",
    "        states, actions, rewards, n_states, dones = self.take_data(self.batch_size)\n",
    "\n",
    "        states = tf.convert_to_tensor(states, dtype= tf.float32)\n",
    "        n_states = tf.convert_to_tensor(n_states, dtype= tf.float32)\n",
    "        rewards = tf.convert_to_tensor(rewards, dtype= tf.float32)\n",
    "        actions = tf.convert_to_tensor(actions, dtype= tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "\n",
    "            target_actions = self.actor_target(n_states)\n",
    "            target_actions += tf.clip_by_value(tf.random.normal(shape=[*np.shape(target_actions)], mean=0.0, stddev=0.2), -0.5, 0.5)\n",
    "            target_actions = self.max_action * (tf.clip_by_value(target_actions, self.min_action, self.max_action))\n",
    "\n",
    "\n",
    "            target_next_state_values = tf.squeeze(self.critic_target(n_states, target_actions), 1)\n",
    "            target_next_state_values2 = tf.squeeze(self.critic_target2(n_states, target_actions), 1)\n",
    "\n",
    "            critic_value = tf.squeeze(self.critic_main(states, actions), 1)\n",
    "            critic_value2 = tf.squeeze(self.critic_main2(states, actions), 1)\n",
    "\n",
    "            next_state_target_value = tf.math.minimum(target_next_state_values, target_next_state_values2)\n",
    "\n",
    "            target_values = rewards + self.gamma * next_state_target_value * dones\n",
    "            critic_loss1 = tf.keras.losses.MSE(target_values, critic_value)\n",
    "            critic_loss2 = tf.keras.losses.MSE(target_values, critic_value2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        grads1 = tape1.gradient(critic_loss1, self.critic_main.trainable_variables)\n",
    "        grads2 = tape2.gradient(critic_loss2, self.critic_main2.trainable_variables)\n",
    "\n",
    "        self.c_opt1.apply_gradients(zip(grads1, self.critic_main.trainable_variables))\n",
    "        self.c_opt2.apply_gradients(zip(grads2, self.critic_main2.trainable_variables))\n",
    "\n",
    "\n",
    "        self.trainstep +=1\n",
    "\n",
    "        if self.trainstep % self.actor_update_steps == 0:\n",
    "\n",
    "            with tf.GradientTape() as tape3:\n",
    "\n",
    "                new_policy_actions = self.actor_main(states)\n",
    "                actor_loss = -self.critic_main(states, new_policy_actions)\n",
    "                actor_loss = tf.math.reduce_mean(actor_loss)\n",
    "\n",
    "            grads3 = tape3.gradient(actor_loss, self.actor_main.trainable_variables)\n",
    "            self.a_opt.apply_gradients(zip(grads3, self.actor_main.trainable_variables))\n",
    "\n",
    "        self.update_target()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min_action: -1.0\n",
      "Max_action:  1.0\n",
      "States:  (2,)\n",
      "Actions:  (1,)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "print('Min_action:', env.action_space.low[0])\n",
    "print('Max_action: ', env.action_space.high[0])\n",
    "print('States: ', env.observation_space.shape)\n",
    "print('Actions: ', env.action_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0  Score: -0.5813277745963521  AVG: 0\n",
      "Episode: 1  Score: -0.05619052600656026  AVG: 0\n",
      "Episode: 2  Score: -0.0834253628722238  AVG: 0\n",
      "Episode: 3  Score: -0.0696188171375836  AVG: 0\n",
      "Episode: 4  Score: -0.03609266589308854  AVG: 0\n",
      "Episode: 5  Score: -0.036014952044915866  AVG: 0\n",
      "Episode: 6  Score: -0.04997331708358718  AVG: 0\n",
      "Episode: 7  Score: -0.07702190496602339  AVG: 0\n",
      "Episode: 8  Score: -0.041531934674486126  AVG: 0\n",
      "Episode: 9  Score: -0.035967310224876474  AVG: 0\n",
      "Episode: 10  Score: -0.053583646538797096  AVG: 0\n",
      "Episode: 11  Score: -0.15525425916005053  AVG: 0\n",
      "Episode: 12  Score: -0.07582186563899365  AVG: 0\n",
      "Episode: 13  Score: -0.09309369933838688  AVG: 0\n",
      "Episode: 14  Score: -0.10018556652598909  AVG: 0\n",
      "Episode: 15  Score: -0.03918442595231199  AVG: 0\n",
      "Episode: 16  Score: -0.03274115772036718  AVG: 0\n",
      "Episode: 17  Score: -0.023580611651674903  AVG: 0\n",
      "Episode: 18  Score: -0.034329764306421096  AVG: 0\n",
      "Episode: 19  Score: -0.05260109253276382  AVG: 0\n",
      "Episode: 20  Score: -0.038187059602399  AVG: 0\n",
      "Episode: 21  Score: -0.04701303955473513  AVG: 0\n",
      "Episode: 22  Score: -0.12116423222192652  AVG: 0\n",
      "Episode: 23  Score: -0.14470452827889177  AVG: 0\n",
      "Episode: 24  Score: -0.08415086036996997  AVG: 0\n",
      "Episode: 25  Score: -0.15635530809845652  AVG: 0\n",
      "Episode: 26  Score: -0.08278443383688688  AVG: 0\n",
      "Episode: 27  Score: -0.04082377418798115  AVG: 0\n",
      "Episode: 28  Score: -0.13883493458051221  AVG: 0\n",
      "Episode: 29  Score: -0.15032301480971338  AVG: 0\n",
      "Episode: 30  Score: -0.11540962916588937  AVG: 0\n",
      "Episode: 31  Score: -0.055880490926774586  AVG: 0\n",
      "Episode: 32  Score: -0.04306993988608103  AVG: 0\n",
      "Episode: 33  Score: -0.09137435527023131  AVG: 0\n",
      "Episode: 34  Score: -0.07845361011224464  AVG: 0\n",
      "Episode: 35  Score: -0.042493155534649996  AVG: 0\n",
      "Episode: 36  Score: -0.17585301809641607  AVG: 0\n",
      "Episode: 37  Score: -0.1056400884392911  AVG: 0\n",
      "Episode: 38  Score: -0.1148442015166394  AVG: 0\n",
      "Episode: 39  Score: -0.14429805035655469  AVG: 0\n",
      "Episode: 40  Score: -0.02961375696897702  AVG: 0\n",
      "Episode: 41  Score: -0.040454226833843025  AVG: 0\n",
      "Episode: 42  Score: -0.023816455695128358  AVG: 0\n",
      "Episode: 43  Score: -0.09536653683492255  AVG: 0\n",
      "Episode: 44  Score: -0.17685014461545034  AVG: 0\n",
      "Episode: 45  Score: -0.10017874142313257  AVG: 0\n",
      "Episode: 46  Score: -0.24894976350218065  AVG: 0\n",
      "Episode: 47  Score: -0.07682175701487208  AVG: 0\n",
      "Episode: 48  Score: -0.014911755821357803  AVG: 0\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(1, -1, 1)\n",
    "\n",
    "n_episodes = 50\n",
    "avg_hist = []\n",
    "scores = []\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    score = 0 \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        n_state, reward, done, _ = env.step(action)\n",
    "        agent.store(state, action, reward, n_state, done)\n",
    "        agent.upgrade()\n",
    "        state = n_state\n",
    "        score += reward\n",
    "    \n",
    "    scores.append(score)\n",
    "    avg_reward = int(np.mean(scores[-100:]))\n",
    "    avg_hist.append(avg_reward)\n",
    "    print(f'Episode: {i}  Score: {score}  AVG: {avg_reward}')\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scores)\n",
    "plt.grid()\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.act(state, True)\n",
    "        n_state, _, done, _ = env.step(action)\n",
    "        state = n_state\n",
    "        env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.actor_main.save_weights('actor_weights.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
