{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-ef76775f4c1a>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "Is GPU available? True\n",
      "TF version: 2.3.1\n",
      "Keras version: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Is GPU available?\", tf.test.is_gpu_available())\n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"Keras version:\", tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MSE\n",
    "\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.f1 = tf.keras.layers.Dense(512, activation='relu')\n",
    "        self.f2 = tf.keras.layers.Dense(512, activation='relu')\n",
    "        self.v =  tf.keras.layers.Dense(1, activation=None)\n",
    "\n",
    "    def call(self, inputstate, action):\n",
    "        x = self.f1(tf.concat([inputstate, action], axis=1))\n",
    "        x = self.f2(x)\n",
    "        x = self.v(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self, no_action):\n",
    "        super(Actor, self).__init__()    \n",
    "        self.f1 = tf.keras.layers.Dense(512, activation='relu')\n",
    "        self.f2 = tf.keras.layers.Dense(512, activation='relu')\n",
    "        self.mu =  tf.keras.layers.Dense(no_action, activation='tanh')\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.f1(state)\n",
    "        x = self.f2(x)\n",
    "        x = self.mu(x)  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, n_actions, min_action, max_action):\n",
    "        self.actor_main = Actor(n_actions)\n",
    "        self.actor_target = Actor(n_actions)\n",
    "        self.critic_main = Critic()\n",
    "        self.critic_main2 = Critic()\n",
    "        self.critic_target = Critic()\n",
    "        self.critic_target2 = Critic()\n",
    "        self.batch_size = 64\n",
    "        self.n_actions = n_actions\n",
    "        self.a_opt = tf.keras.optimizers.Adam(0.001)\n",
    "        self.c_opt1 = tf.keras.optimizers.Adam(0.002)\n",
    "        self.c_opt2 = tf.keras.optimizers.Adam(0.002)\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.trainstep = 0\n",
    "        self.gamma = 0.99\n",
    "        self.min_action = min_action\n",
    "        self.max_action = max_action\n",
    "        self.actor_update_steps = 2\n",
    "        self.warmup = 200\n",
    "        self.actor_target.compile(optimizer=self.a_opt)\n",
    "        self.critic_target.compile(optimizer=self.c_opt1)\n",
    "        self.critic_target2.compile(optimizer=self.c_opt2)\n",
    "        self.tau = 0.005\n",
    "\n",
    "\n",
    "    def act(self, state, evaluate=False):\n",
    "        if self.trainstep > self.warmup:\n",
    "            evaluate = True\n",
    "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "        actions = self.actor_main(state)\n",
    "        if not evaluate:\n",
    "            actions += tf.random.normal(shape=[self.n_actions], mean=0.0, stddev=0.1)\n",
    "\n",
    "        actions = self.max_action * (tf.clip_by_value(actions, self.min_action, self.max_action))\n",
    "        \n",
    "        return actions[0]\n",
    "    \n",
    "    def store(self, state, action, reward, n_state, done):\n",
    "        pack = [state, action, reward, n_state, 1 - int(done)]\n",
    "        self.memory.append(pack)\n",
    "    \n",
    "    def take_data(self, batch_size):\n",
    "        pack = random.sample(self.memory, batch_size)\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        n_states = []\n",
    "        dones = []\n",
    "        for i in range(batch_size):\n",
    "            states.append(pack[i][0])\n",
    "            actions.append(pack[i][1])\n",
    "            rewards.append(pack[i][2])\n",
    "            n_states.append(pack[i][3])\n",
    "            dones.append(pack[i][4])\n",
    "        return states, actions, rewards, n_states, dones\n",
    "\n",
    "    def update_target(self, tau=None):\n",
    "\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "\n",
    "        weights1 = []\n",
    "        targets1 = self.actor_target.weights\n",
    "        for i, weight in enumerate(self.actor_main.weights):\n",
    "            weights1.append(weight * tau + targets1[i]*(1-tau))\n",
    "        self.actor_target.set_weights(weights1)\n",
    "\n",
    "        weights2 = []\n",
    "        targets2 = self.critic_target.weights\n",
    "        for i, weight in enumerate(self.critic_main.weights):\n",
    "            weights2.append(weight * tau + targets2[i]*(1-tau))\n",
    "        self.critic_target.set_weights(weights2)\n",
    "\n",
    "\n",
    "        weights3 = []\n",
    "        targets3 = self.critic_target2.weights\n",
    "        for i, weight in enumerate(self.critic_main2.weights):\n",
    "            weights3.append(weight * tau + targets3[i]*(1-tau))\n",
    "        self.critic_target2.set_weights(weights3)\n",
    "    \n",
    "    def upgrade(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return \n",
    "\n",
    "        states, actions, rewards, n_states, dones = self.take_data(self.batch_size)\n",
    "\n",
    "        states = tf.convert_to_tensor(states, dtype= tf.float32)\n",
    "        n_states = tf.convert_to_tensor(n_states, dtype= tf.float32)\n",
    "        rewards = tf.convert_to_tensor(rewards, dtype= tf.float32)\n",
    "        actions = tf.convert_to_tensor(actions, dtype= tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "\n",
    "            target_actions = self.actor_target(n_states)\n",
    "            target_actions += tf.clip_by_value(tf.random.normal(shape=[*np.shape(target_actions)], mean=0.0, stddev=0.2), -0.5, 0.5)\n",
    "            target_actions = self.max_action * (tf.clip_by_value(target_actions, self.min_action, self.max_action))\n",
    "\n",
    "\n",
    "            target_next_state_values = tf.squeeze(self.critic_target(n_states, target_actions), 1)\n",
    "            target_next_state_values2 = tf.squeeze(self.critic_target2(n_states, target_actions), 1)\n",
    "\n",
    "            critic_value = tf.squeeze(self.critic_main(states, actions), 1)\n",
    "            critic_value2 = tf.squeeze(self.critic_main2(states, actions), 1)\n",
    "\n",
    "            next_state_target_value = tf.math.minimum(target_next_state_values, target_next_state_values2)\n",
    "\n",
    "            target_values = rewards + self.gamma * next_state_target_value * dones\n",
    "            critic_loss1 = tf.keras.losses.MSE(target_values, critic_value)\n",
    "            critic_loss2 = tf.keras.losses.MSE(target_values, critic_value2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        grads1 = tape1.gradient(critic_loss1, self.critic_main.trainable_variables)\n",
    "        grads2 = tape2.gradient(critic_loss2, self.critic_main2.trainable_variables)\n",
    "\n",
    "        self.c_opt1.apply_gradients(zip(grads1, self.critic_main.trainable_variables))\n",
    "        self.c_opt2.apply_gradients(zip(grads2, self.critic_main2.trainable_variables))\n",
    "\n",
    "\n",
    "        self.trainstep +=1\n",
    "\n",
    "        if self.trainstep % self.actor_update_steps == 0:\n",
    "\n",
    "            with tf.GradientTape() as tape3:\n",
    "\n",
    "                new_policy_actions = self.actor_main(states)\n",
    "                actor_loss = -self.critic_main(states, new_policy_actions)\n",
    "                actor_loss = tf.math.reduce_mean(actor_loss)\n",
    "\n",
    "            grads3 = tape3.gradient(actor_loss, self.actor_main.trainable_variables)\n",
    "            self.a_opt.apply_gradients(zip(grads3, self.actor_main.trainable_variables))\n",
    "\n",
    "        self.update_target()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min_action: -2.0\n",
      "Max_action:  2.0\n",
      "States:  (3,)\n",
      "Actions:  (1,)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "\n",
    "print('Min_action:', env.action_space.low[0])\n",
    "print('Max_action: ', env.action_space.high[0])\n",
    "print('States: ', env.observation_space.shape)\n",
    "print('Actions: ', env.action_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0  Score: -1277.582984186938  AVG: -1277\n",
      "Episode: 1  Score: -1359.3737342375377  AVG: -1318\n",
      "Episode: 2  Score: -1484.0820821683342  AVG: -1373\n",
      "Episode: 3  Score: -1477.5558498192827  AVG: -1399\n",
      "Episode: 4  Score: -1512.030338450466  AVG: -1422\n",
      "Episode: 5  Score: -1502.7540986275262  AVG: -1435\n",
      "Episode: 6  Score: -1507.958339565094  AVG: -1445\n",
      "Episode: 7  Score: -1296.678735374606  AVG: -1427\n",
      "Episode: 8  Score: -1493.7162050107383  AVG: -1434\n",
      "Episode: 9  Score: -1518.0516660089402  AVG: -1442\n",
      "Episode: 10  Score: -1504.392950215273  AVG: -1448\n",
      "Episode: 11  Score: -1247.3453079194048  AVG: -1431\n",
      "Episode: 12  Score: -729.3280810362421  AVG: -1377\n",
      "Episode: 13  Score: -981.2209049297322  AVG: -1349\n",
      "Episode: 14  Score: -390.0116187497333  AVG: -1285\n",
      "Episode: 15  Score: -676.8100769477404  AVG: -1247\n",
      "Episode: 16  Score: -263.05852819902833  AVG: -1189\n",
      "Episode: 17  Score: -3.1188557486539796  AVG: -1123\n",
      "Episode: 18  Score: -1358.8724587422262  AVG: -1135\n",
      "Episode: 19  Score: -789.9421729932753  AVG: -1118\n",
      "Episode: 20  Score: -516.166912452079  AVG: -1090\n",
      "Episode: 21  Score: -128.09755229060602  AVG: -1046\n",
      "Episode: 22  Score: -120.50021021789578  AVG: -1006\n",
      "Episode: 23  Score: -125.12625675243395  AVG: -969\n",
      "Episode: 24  Score: -250.48623910041601  AVG: -940\n",
      "Episode: 25  Score: -245.03099205524524  AVG: -913\n",
      "Episode: 26  Score: -123.85809195175462  AVG: -884\n",
      "Episode: 27  Score: -248.8579619848888  AVG: -861\n",
      "Episode: 28  Score: -127.6606598284798  AVG: -836\n",
      "Episode: 29  Score: -233.98370178702766  AVG: -816\n",
      "Episode: 30  Score: -127.89135537743402  AVG: -794\n",
      "Episode: 31  Score: -2.74318794897696  AVG: -769\n",
      "Episode: 32  Score: -118.85368519945898  AVG: -749\n",
      "Episode: 33  Score: -125.46443757514999  AVG: -731\n",
      "Episode: 34  Score: -358.9408864716282  AVG: -720\n",
      "Episode: 35  Score: -245.12705827367552  AVG: -707\n",
      "Episode: 36  Score: -118.3710071074929  AVG: -691\n",
      "Episode: 37  Score: -4.680934389602281  AVG: -673\n",
      "Episode: 38  Score: -3.6923992307885847  AVG: -656\n",
      "Episode: 39  Score: -119.66569701431861  AVG: -642\n",
      "Episode: 40  Score: -121.5581011030549  AVG: -630\n",
      "Episode: 41  Score: -124.17140477742555  AVG: -618\n",
      "Episode: 42  Score: -234.13649971271994  AVG: -609\n",
      "Episode: 43  Score: -245.52629473098943  AVG: -601\n",
      "Episode: 44  Score: -118.30063710771954  AVG: -590\n",
      "Episode: 45  Score: -123.495889062711  AVG: -580\n",
      "Episode: 46  Score: -128.7941071784992  AVG: -570\n",
      "Episode: 47  Score: -333.1832748257618  AVG: -565\n",
      "Episode: 48  Score: -229.9010952305852  AVG: -558\n",
      "Episode: 49  Score: -121.75155654502638  AVG: -549\n",
      "Episode: 50  Score: -122.61635402781096  AVG: -541\n",
      "Episode: 51  Score: -233.0999632535526  AVG: -535\n",
      "Episode: 52  Score: -243.8372846781632  AVG: -530\n",
      "Episode: 53  Score: -118.81879208249225  AVG: -522\n",
      "Episode: 54  Score: -121.16278640692374  AVG: -515\n",
      "Episode: 55  Score: -277.6862394577517  AVG: -511\n",
      "Episode: 56  Score: -121.71707543696331  AVG: -504\n",
      "Episode: 57  Score: -122.90201224811523  AVG: -497\n",
      "Episode: 58  Score: -243.82778734894111  AVG: -493\n",
      "Episode: 59  Score: -2.0344901262475688  AVG: -485\n",
      "Episode: 60  Score: -235.32728246544423  AVG: -481\n",
      "Episode: 61  Score: -239.6772766783637  AVG: -477\n",
      "Episode: 62  Score: -115.65016971916292  AVG: -471\n",
      "Episode: 63  Score: -121.09955349866246  AVG: -465\n",
      "Episode: 64  Score: -324.1556691618231  AVG: -463\n",
      "Episode: 65  Score: -119.2537378156849  AVG: -458\n",
      "Episode: 66  Score: -124.87279211692446  AVG: -453\n",
      "Episode: 67  Score: -117.25314565294262  AVG: -448\n",
      "Episode: 68  Score: -227.97954429115606  AVG: -445\n",
      "Episode: 69  Score: -122.15636541929888  AVG: -440\n",
      "Episode: 70  Score: -3.5173033021644344  AVG: -434\n",
      "Episode: 71  Score: -121.99240666659783  AVG: -430\n",
      "Episode: 72  Score: -236.9931686731589  AVG: -427\n",
      "Episode: 73  Score: -234.79097419058746  AVG: -425\n",
      "Episode: 74  Score: -321.0575229986356  AVG: -423\n",
      "Episode: 75  Score: -237.6608113562568  AVG: -421\n",
      "Episode: 76  Score: -121.22029345893127  AVG: -417\n",
      "Episode: 77  Score: -3.2610642825132254  AVG: -411\n",
      "Episode: 78  Score: -118.58823357932705  AVG: -408\n",
      "Episode: 79  Score: -226.3050074094587  AVG: -406\n",
      "Episode: 80  Score: -125.91059841239124  AVG: -402\n",
      "Episode: 81  Score: -233.80923993428806  AVG: -400\n",
      "Episode: 82  Score: -122.42188381002433  AVG: -397\n",
      "Episode: 83  Score: -122.80510044429738  AVG: -393\n",
      "Episode: 84  Score: -129.65110610475253  AVG: -390\n",
      "Episode: 85  Score: -243.42274086624482  AVG: -389\n",
      "Episode: 86  Score: -121.02495772842516  AVG: -385\n",
      "Episode: 87  Score: -119.99128362561174  AVG: -382\n",
      "Episode: 88  Score: -10.13585186948976  AVG: -378\n",
      "Episode: 89  Score: -131.48800064821404  AVG: -376\n",
      "Episode: 90  Score: -239.47300850812718  AVG: -374\n",
      "Episode: 91  Score: -357.9300102201614  AVG: -374\n",
      "Episode: 92  Score: -127.40944893980117  AVG: -371\n",
      "Episode: 93  Score: -124.51983173025427  AVG: -369\n",
      "Episode: 94  Score: -348.848966278339  AVG: -368\n",
      "Episode: 95  Score: -132.6177983177855  AVG: -366\n",
      "Episode: 96  Score: -244.26559917077196  AVG: -365\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(1, -2, 2)\n",
    "\n",
    "n_episodes = 2000\n",
    "avg_hist = []\n",
    "scores = []\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    score = 0 \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        n_state, reward, done, _ = env.step(action)\n",
    "        agent.store(state, action, reward, n_state, done)\n",
    "        agent.upgrade()\n",
    "        state = n_state\n",
    "        score += reward\n",
    "    \n",
    "    scores.append(score)\n",
    "    avg_reward = int(np.mean(scores[-100:]))\n",
    "    avg_hist.append(avg_reward)\n",
    "    print(f'Episode: {i}  Score: {score}  AVG: {avg_reward}')\n",
    "    \n",
    "    if avg_reward >= 210:\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(avg_hist)\n",
    "plt.grid()\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Avg rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.act(state, True)\n",
    "        n_state, _, done, _ = env.step(action)\n",
    "        state = n_state\n",
    "        env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.actor_main.save_weights('actor_weights.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
