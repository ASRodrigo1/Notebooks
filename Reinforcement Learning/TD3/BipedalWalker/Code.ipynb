{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-ef76775f4c1a>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "Is GPU available? True\n",
      "TF version: 2.3.1\n",
      "Keras version: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Is GPU available?\", tf.test.is_gpu_available())\n",
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"Keras version:\", tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MSE\n",
    "\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        self.f1 = tf.keras.layers.Dense(512, activation='relu')\n",
    "        self.f2 = tf.keras.layers.Dense(512, activation='relu')\n",
    "        self.v =  tf.keras.layers.Dense(1, activation=None)\n",
    "\n",
    "    def call(self, inputstate, action):\n",
    "        x = self.f1(tf.concat([inputstate, action], axis=1))\n",
    "        x = self.f2(x)\n",
    "        x = self.v(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self, no_action):\n",
    "        super(Actor, self).__init__()    \n",
    "        self.f1 = tf.keras.layers.Dense(512, activation='relu')\n",
    "        self.f2 = tf.keras.layers.Dense(512, activation='relu')\n",
    "        self.mu =  tf.keras.layers.Dense(no_action, activation='tanh')\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.f1(state)\n",
    "        x = self.f2(x)\n",
    "        x = self.mu(x)  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, n_actions, min_action, max_action):\n",
    "        self.actor_main = Actor(n_actions)\n",
    "        self.actor_target = Actor(n_actions)\n",
    "        self.critic_main = Critic()\n",
    "        self.critic_main2 = Critic()\n",
    "        self.critic_target = Critic()\n",
    "        self.critic_target2 = Critic()\n",
    "        self.batch_size = 64\n",
    "        self.n_actions = n_actions\n",
    "        self.a_opt = tf.keras.optimizers.Adam(0.001)\n",
    "        self.c_opt1 = tf.keras.optimizers.Adam(0.002)\n",
    "        self.c_opt2 = tf.keras.optimizers.Adam(0.002)\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.trainstep = 0\n",
    "        self.gamma = 0.99\n",
    "        self.min_action = min_action\n",
    "        self.max_action = max_action\n",
    "        self.actor_update_steps = 2\n",
    "        self.warmup = 200\n",
    "        self.actor_target.compile(optimizer=self.a_opt)\n",
    "        self.critic_target.compile(optimizer=self.c_opt1)\n",
    "        self.critic_target2.compile(optimizer=self.c_opt2)\n",
    "        self.tau = 0.005\n",
    "\n",
    "\n",
    "    def act(self, state, evaluate=False):\n",
    "        if self.trainstep > self.warmup:\n",
    "            evaluate = True\n",
    "        state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "        actions = self.actor_main(state)\n",
    "        if not evaluate:\n",
    "            actions += tf.random.normal(shape=[self.n_actions], mean=0.0, stddev=0.1)\n",
    "\n",
    "        actions = self.max_action * (tf.clip_by_value(actions, self.min_action, self.max_action))\n",
    "        \n",
    "        return actions[0]\n",
    "    \n",
    "    def store(self, state, action, reward, n_state, done):\n",
    "        pack = [state, action, reward, n_state, 1 - int(done)]\n",
    "        self.memory.append(pack)\n",
    "    \n",
    "    def take_data(self, batch_size):\n",
    "        pack = random.sample(self.memory, batch_size)\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        n_states = []\n",
    "        dones = []\n",
    "        for i in range(batch_size):\n",
    "            states.append(pack[i][0])\n",
    "            actions.append(pack[i][1])\n",
    "            rewards.append(pack[i][2])\n",
    "            n_states.append(pack[i][3])\n",
    "            dones.append(pack[i][4])\n",
    "        return states, actions, rewards, n_states, dones\n",
    "\n",
    "    def update_target(self, tau=None):\n",
    "\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "\n",
    "        weights1 = []\n",
    "        targets1 = self.actor_target.weights\n",
    "        for i, weight in enumerate(self.actor_main.weights):\n",
    "            weights1.append(weight * tau + targets1[i]*(1-tau))\n",
    "        self.actor_target.set_weights(weights1)\n",
    "\n",
    "        weights2 = []\n",
    "        targets2 = self.critic_target.weights\n",
    "        for i, weight in enumerate(self.critic_main.weights):\n",
    "            weights2.append(weight * tau + targets2[i]*(1-tau))\n",
    "        self.critic_target.set_weights(weights2)\n",
    "\n",
    "\n",
    "        weights3 = []\n",
    "        targets3 = self.critic_target2.weights\n",
    "        for i, weight in enumerate(self.critic_main2.weights):\n",
    "            weights3.append(weight * tau + targets3[i]*(1-tau))\n",
    "        self.critic_target2.set_weights(weights3)\n",
    "    \n",
    "    def upgrade(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return \n",
    "\n",
    "        states, actions, rewards, n_states, dones = self.take_data(self.batch_size)\n",
    "\n",
    "        states = tf.convert_to_tensor(states, dtype= tf.float32)\n",
    "        n_states = tf.convert_to_tensor(n_states, dtype= tf.float32)\n",
    "        rewards = tf.convert_to_tensor(rewards, dtype= tf.float32)\n",
    "        actions = tf.convert_to_tensor(actions, dtype= tf.float32)\n",
    "\n",
    "        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "\n",
    "            target_actions = self.actor_target(n_states)\n",
    "            target_actions += tf.clip_by_value(tf.random.normal(shape=[*np.shape(target_actions)], mean=0.0, stddev=0.2), -0.5, 0.5)\n",
    "            target_actions = self.max_action * (tf.clip_by_value(target_actions, self.min_action, self.max_action))\n",
    "\n",
    "\n",
    "            target_next_state_values = tf.squeeze(self.critic_target(n_states, target_actions), 1)\n",
    "            target_next_state_values2 = tf.squeeze(self.critic_target2(n_states, target_actions), 1)\n",
    "\n",
    "            critic_value = tf.squeeze(self.critic_main(states, actions), 1)\n",
    "            critic_value2 = tf.squeeze(self.critic_main2(states, actions), 1)\n",
    "\n",
    "            next_state_target_value = tf.math.minimum(target_next_state_values, target_next_state_values2)\n",
    "\n",
    "            target_values = rewards + self.gamma * next_state_target_value * dones\n",
    "            critic_loss1 = tf.keras.losses.MSE(target_values, critic_value)\n",
    "            critic_loss2 = tf.keras.losses.MSE(target_values, critic_value2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        grads1 = tape1.gradient(critic_loss1, self.critic_main.trainable_variables)\n",
    "        grads2 = tape2.gradient(critic_loss2, self.critic_main2.trainable_variables)\n",
    "\n",
    "        self.c_opt1.apply_gradients(zip(grads1, self.critic_main.trainable_variables))\n",
    "        self.c_opt2.apply_gradients(zip(grads2, self.critic_main2.trainable_variables))\n",
    "\n",
    "\n",
    "        self.trainstep +=1\n",
    "\n",
    "        if self.trainstep % self.actor_update_steps == 0:\n",
    "\n",
    "            with tf.GradientTape() as tape3:\n",
    "\n",
    "                new_policy_actions = self.actor_main(states)\n",
    "                actor_loss = -self.critic_main(states, new_policy_actions)\n",
    "                actor_loss = tf.math.reduce_mean(actor_loss)\n",
    "\n",
    "            grads3 = tape3.gradient(actor_loss, self.actor_main.trainable_variables)\n",
    "            self.a_opt.apply_gradients(zip(grads3, self.actor_main.trainable_variables))\n",
    "\n",
    "        self.update_target()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min_action: -1.0\n",
      "Max_action:  1.0\n",
      "States:  (24,)\n",
      "Actions:  (4,)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('BipedalWalker-v3')\n",
    "\n",
    "print('Min_action:', env.action_space.low[0])\n",
    "print('Max_action: ', env.action_space.high[0])\n",
    "print('States: ', env.observation_space.shape)\n",
    "print('Actions: ', env.action_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0  Score: -125.60700541748976  AVG: -125\n",
      "Episode: 1  Score: -132.0630005510909  AVG: -128\n",
      "Episode: 2  Score: -125.23540681797017  AVG: -127\n",
      "Episode: 3  Score: -134.67105365155763  AVG: -129\n",
      "Episode: 4  Score: -109.49355492295139  AVG: -125\n",
      "Episode: 5  Score: -102.26208222785654  AVG: -121\n",
      "Episode: 6  Score: -103.46943957644453  AVG: -118\n",
      "Episode: 7  Score: -102.9230345956888  AVG: -116\n",
      "Episode: 8  Score: -103.23445432660115  AVG: -115\n",
      "Episode: 9  Score: -101.88908364009546  AVG: -114\n",
      "Episode: 10  Score: -102.29794096462429  AVG: -113\n",
      "Episode: 11  Score: -102.48651198707893  AVG: -112\n",
      "Episode: 12  Score: -102.5076678015062  AVG: -111\n",
      "Episode: 13  Score: -102.50890868668196  AVG: -110\n",
      "Episode: 14  Score: -103.85358734387532  AVG: -110\n",
      "Episode: 15  Score: -102.35183625892053  AVG: -109\n",
      "Episode: 16  Score: -102.19216968285913  AVG: -109\n",
      "Episode: 17  Score: -101.85229631065764  AVG: -108\n",
      "Episode: 18  Score: -102.279531875796  AVG: -108\n",
      "Episode: 19  Score: -101.80706813934259  AVG: -108\n",
      "Episode: 20  Score: -124.42661981580903  AVG: -109\n",
      "Episode: 21  Score: -106.37069491345746  AVG: -108\n",
      "Episode: 22  Score: -102.24771870077525  AVG: -108\n",
      "Episode: 23  Score: -110.67304641949075  AVG: -108\n",
      "Episode: 24  Score: -101.38162903222255  AVG: -108\n",
      "Episode: 25  Score: -104.92671559702356  AVG: -108\n",
      "Episode: 26  Score: -110.75688918767186  AVG: -108\n",
      "Episode: 27  Score: -110.53320675959873  AVG: -108\n",
      "Episode: 28  Score: -100.83159093404747  AVG: -108\n",
      "Episode: 29  Score: -103.08971990271472  AVG: -108\n",
      "Episode: 30  Score: -101.30547578788176  AVG: -107\n",
      "Episode: 31  Score: -105.19299970960927  AVG: -107\n",
      "Episode: 32  Score: -106.40935661904638  AVG: -107\n",
      "Episode: 33  Score: -149.5242816382659  AVG: -108\n",
      "Episode: 34  Score: -204.1327329269154  AVG: -111\n",
      "Episode: 35  Score: -127.36505355645374  AVG: -112\n",
      "Episode: 36  Score: -127.58324772271078  AVG: -112\n",
      "Episode: 37  Score: -127.29062481525416  AVG: -112\n",
      "Episode: 38  Score: -132.24179892242017  AVG: -113\n",
      "Episode: 39  Score: -132.1670228028757  AVG: -113\n",
      "Episode: 40  Score: -124.37347921593778  AVG: -114\n",
      "Episode: 41  Score: -124.93860151661379  AVG: -114\n",
      "Episode: 42  Score: -134.9962204106537  AVG: -114\n",
      "Episode: 43  Score: -128.95284136219695  AVG: -115\n",
      "Episode: 44  Score: -114.66612379959349  AVG: -115\n",
      "Episode: 45  Score: -111.2427427324485  AVG: -115\n",
      "Episode: 46  Score: -111.13078392226684  AVG: -114\n",
      "Episode: 47  Score: -128.96817778262434  AVG: -115\n",
      "Episode: 48  Score: -102.69465992479212  AVG: -115\n",
      "Episode: 49  Score: -103.41376438887168  AVG: -114\n",
      "Episode: 50  Score: -104.26879360442733  AVG: -114\n",
      "Episode: 51  Score: -99.24831218990559  AVG: -114\n",
      "Episode: 52  Score: -171.81897146196462  AVG: -115\n",
      "Episode: 53  Score: -122.94152145434171  AVG: -115\n",
      "Episode: 54  Score: -107.60350717192081  AVG: -115\n",
      "Episode: 55  Score: -107.64000510829067  AVG: -115\n",
      "Episode: 56  Score: -126.9070831227613  AVG: -115\n",
      "Episode: 57  Score: -120.920966386086  AVG: -115\n",
      "Episode: 58  Score: -130.36041516295634  AVG: -115\n",
      "Episode: 59  Score: -111.12354852706939  AVG: -115\n",
      "Episode: 60  Score: -115.15709531719672  AVG: -115\n",
      "Episode: 61  Score: -134.02123750018885  AVG: -115\n",
      "Episode: 62  Score: -130.36627037149202  AVG: -116\n",
      "Episode: 63  Score: -101.86440715957991  AVG: -115\n",
      "Episode: 64  Score: -122.63611596936794  AVG: -116\n",
      "Episode: 65  Score: -103.7513180466524  AVG: -115\n",
      "Episode: 66  Score: -100.18564034193832  AVG: -115\n",
      "Episode: 67  Score: -102.28053123006302  AVG: -115\n",
      "Episode: 68  Score: -114.48486676814096  AVG: -115\n",
      "Episode: 69  Score: -101.06354715290153  AVG: -115\n",
      "Episode: 70  Score: -102.32456286359267  AVG: -115\n",
      "Episode: 71  Score: -101.11506145686252  AVG: -114\n",
      "Episode: 72  Score: -102.77075621043083  AVG: -114\n",
      "Episode: 73  Score: -101.46799345257382  AVG: -114\n",
      "Episode: 74  Score: -102.28681428514669  AVG: -114\n",
      "Episode: 75  Score: -102.94384555158206  AVG: -114\n",
      "Episode: 76  Score: -103.13614046318891  AVG: -114\n",
      "Episode: 77  Score: -102.3405237973753  AVG: -113\n",
      "Episode: 78  Score: -102.20206682582882  AVG: -113\n",
      "Episode: 79  Score: -103.68544169904975  AVG: -113\n",
      "Episode: 80  Score: -113.70091653608935  AVG: -113\n",
      "Episode: 81  Score: -101.34772856935611  AVG: -113\n",
      "Episode: 82  Score: -106.83383298672922  AVG: -113\n",
      "Episode: 83  Score: -102.28597252280265  AVG: -113\n",
      "Episode: 84  Score: -114.58505891020224  AVG: -113\n",
      "Episode: 85  Score: -107.73810920369439  AVG: -113\n",
      "Episode: 86  Score: -190.99265024478143  AVG: -114\n",
      "Episode: 87  Score: -101.58369586731276  AVG: -113\n",
      "Episode: 88  Score: -107.54333376448633  AVG: -113\n",
      "Episode: 89  Score: -111.6718289306541  AVG: -113\n",
      "Episode: 90  Score: -108.14388782918702  AVG: -113\n",
      "Episode: 91  Score: -127.0596143987396  AVG: -113\n",
      "Episode: 92  Score: -111.06558238879079  AVG: -113\n",
      "Episode: 93  Score: -120.57779293403756  AVG: -114\n",
      "Episode: 94  Score: -114.84977394270152  AVG: -114\n",
      "Episode: 95  Score: -107.57771359794276  AVG: -113\n",
      "Episode: 96  Score: -103.659909923345  AVG: -113\n",
      "Episode: 97  Score: -141.28471374222048  AVG: -114\n",
      "Episode: 98  Score: -107.65033064151618  AVG: -114\n",
      "Episode: 99  Score: -131.25221592104498  AVG: -114\n",
      "Episode: 100  Score: -116.25519851718781  AVG: -114\n",
      "Episode: 101  Score: -108.53470967618065  AVG: -113\n",
      "Episode: 102  Score: -104.25332679197814  AVG: -113\n",
      "Episode: 103  Score: -130.67360707017593  AVG: -113\n",
      "Episode: 104  Score: -109.17985805892944  AVG: -113\n",
      "Episode: 105  Score: -103.98980381433822  AVG: -113\n",
      "Episode: 106  Score: -164.1745259452943  AVG: -114\n",
      "Episode: 107  Score: -155.5880384137038  AVG: -114\n",
      "Episode: 108  Score: -146.68349532326513  AVG: -115\n",
      "Episode: 109  Score: -107.73918750025207  AVG: -115\n",
      "Episode: 110  Score: -122.73262625630574  AVG: -115\n",
      "Episode: 111  Score: -169.5429861533865  AVG: -116\n",
      "Episode: 112  Score: -104.4525512598219  AVG: -116\n",
      "Episode: 113  Score: -142.16405470276086  AVG: -116\n",
      "Episode: 114  Score: -178.3658175017816  AVG: -117\n",
      "Episode: 115  Score: -167.5879520442731  AVG: -117\n",
      "Episode: 116  Score: -127.29239794026915  AVG: -118\n",
      "Episode: 117  Score: -131.22845671599853  AVG: -118\n",
      "Episode: 118  Score: -131.35332855336875  AVG: -118\n",
      "Episode: 119  Score: -141.41860228224547  AVG: -119\n",
      "Episode: 120  Score: -159.90402959551824  AVG: -119\n",
      "Episode: 121  Score: -104.39831863796044  AVG: -119\n",
      "Episode: 122  Score: -117.07052331798212  AVG: -119\n",
      "Episode: 123  Score: -111.51417911158379  AVG: -119\n",
      "Episode: 124  Score: -155.01709158940636  AVG: -120\n",
      "Episode: 125  Score: -102.4197036881711  AVG: -120\n",
      "Episode: 126  Score: -155.17410479880309  AVG: -120\n",
      "Episode: 127  Score: -166.41113351284216  AVG: -121\n",
      "Episode: 128  Score: -178.9378975331399  AVG: -122\n",
      "Episode: 129  Score: -173.64971389067793  AVG: -122\n",
      "Episode: 130  Score: -171.99388684949594  AVG: -123\n",
      "Episode: 131  Score: -152.11122889587207  AVG: -123\n",
      "Episode: 132  Score: -158.92424984052215  AVG: -124\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ccbf5f947990>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mn_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupgrade\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-87a495dc8bea>\u001b[0m in \u001b[0;36mupgrade\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mtarget_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mtarget_actions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_by_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mtarget_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_action\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_by_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    860\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v2_behavior\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tf/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mvalue\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;34m\"\"\"The value of this dimension, or None if it is unknown.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = Agent(4, -1, 1)\n",
    "\n",
    "n_episodes = 2000\n",
    "avg_hist = []\n",
    "scores = []\n",
    "steps = []\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    score = 0 \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        n_state, reward, done, _ = env.step(action)\n",
    "        agent.store(state, action, reward, n_state, done)\n",
    "        agent.upgrade()\n",
    "        state = n_state\n",
    "        score += reward\n",
    "        steps.append(reward)\n",
    "    \n",
    "    scores.append(score)\n",
    "    avg_reward = int(np.mean(scores[-100:]))\n",
    "    avg_hist.append(avg_reward)\n",
    "    print(f'Episode: {i}  Score: {score}  AVG: {avg_reward}')\n",
    "    \n",
    "    if np.sum(steps[-1600:]) >= 300:\n",
    "        print('Solved!')\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(avg_hist)\n",
    "plt.grid()\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Avg rewards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('BipedalWalker-v3')\n",
    "\n",
    "for i in range(10):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.act(state, True)\n",
    "        #action = env.action_space.sample()\n",
    "        n_state, _, done, _ = env.step(action)\n",
    "        state = n_state\n",
    "        env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.actor_main.save_weights('actor_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
